\chapter{Introduction}\label{Introduction}

Our understanding of the Universe has advanced with the advancement in our 
theoretical, observational and computational abilities in the last two decades. 
While we are confident about the flatness of the Universe from CMB experiments, 
the indicative accelerating Universe from Supernovae-Ia surveys is well supported
by a cosmological constant. Finally a non-interacting dark matter is essential 
to explain the kinematics of galaxies in clusters, lightcurves of galaxies, 
gravitational lensing etc. A comprehensive model, the so-called $\Lambda$CDM,
is very successful in explaning most of the observations independently and 
combined constraints tells us that our Universe is composed of about 70$\%$
dark-energy in the form of cosmological constant $\Lambda$, about 25$\%$ non-interacting
cold dark-matter (CDM) and only 5$\%$ of the baryonic/ordinary matter. 

%------------------------------------------------------------------------------



\section{An Expanding Universe}

A stationary Universe would be boring, we have an expanding one - even better its
accelerating. The evidences are found in various probes like SNe-Ia \cite{}, 
CMB \cite{}, BAO \cite{}, galaxy clustering \cite{}, Lyman alpha \cite{} etc. 
The state-of-the-art was seeded when somebody sees something \cite{} which is 
further improved/informed by someone else \cite{}. 

Adding the so-called {\it cosmological principle} to the Einstein's field equations, 
which relates the Einstein tensor (geometrical object) to the Energy-Momentum tensor, 
derives the Friedmann equations:

\begin{equation}
\centering
\left( \dfrac{\dot{a}}{a} \right)^2 = \dfrac{8 \pi G}{3} \rho - \dfrac{Kc^2}{a^2},
\label{eqn:fe1}
\end{equation}

\begin{equation}
\centering
\left( \dfrac{\ddot{a}}{a} \right) = -\dfrac{4 \pi G}{3} \left( \rho + \dfrac{3p}{c^2} \right),
\label{eqn:fe2}
\end{equation}
\\
where, $\rho$ and $p$ are the density and pressure of the corressponding component of
the energy density of the Universe, $a$ is the scale factor for the size of the expanding
Universe normalised to unity at present ($a_0=1$).
$K$ is the curvature parameter and follows $K=0$ for a flat Universe. $G$ and $c$ are
the Gravitation constant and speed of light respectively. Related by equation \ref{eqn:fe1} 
and \ref{eqn:fe2}, the dynamics and expansion of the Universe depend on its contents. 
Combining these two equations give a third equation, also known as {\it third} Friedmann
equation:

\begin{equation}
\centering
\dot{\rho} = -3 \dfrac{\dot{a}}{a}(\rho + p).
\label{eqn:fe3}
\end{equation}

These three equations completely describe the dynamics of the Universe at all times.
The first two equations relates the matter variables ($\rho$ and $p$) to the geometric
variables ($a$ and $K$) whereas the third equation resembles the first law of 
thermodynamics for adiabatically expanding Universe, fairly approximated.

One can also relate $\rho$ and $p$ with an {\it equation of state}:

\begin{equation}
\centering
p = w \rho,
\label{eqn:eos}
\end{equation}
\\
$w$ is known as the equation of state variable for the respective component
of the energy density of Universe. For non-relativistic matter like dust $w=0$,
for photons and relativistic neutrinos $w=1/3$ whereas for a cosmological constant
$w=-1$. Analysing equation \ref{eqn:fe2}, suggests that $w<-1/3$ is essential to
have an accelerating Universe ($\ddot{a}>0$) which is the general case for the
dark-energy component with cosmological constant as a special case when $w=-1$.

Combining equation \ref{eqn:fe3} and \ref{eqn:eos} for a constant $w$, we get:

\begin{equation}
	\rho(a) = \rho a^{3(1+w)}
	\label{eqn:rho_t}
\end{equation}
\\
giving the evolution of the densities of different components driven by their 
respective equation of state. From now onwards we define $w$ to be the equation of 
state of dark-energy unless stated otherwise.

The curvature parameter $K$ in equation \ref{eqn:fe1} changes sign with the expansion 
rate, therefore $K=0$ is a limiting case. Employing equation \ref{eqn:fe1} today with
$K=0$ yields,

\begin{equation}
	\rho_{cr} \equiv \rho_0 = \dfrac{3H_0^2}{8\pi G}
\end{equation}
\\
Where, we define the expansion rate of the Universe as:
\begin{equation}
	H(t) \equiv \dfrac{\dot{a}}{a}; H_0 \equiv H(t=0) = \dot{a}(t_0)
\end{equation}
\\
where, $H_0$ is the Hubble parameter today or also referred as Hubble constant. In 
practice it is parameterized as $h$ in units of 100 km/s/Mpc.

The critical density $\rho_{cr}$ is a characteristic density of the Universe and is
used to scale the densities of various components as,

\begin{equation}
	\Omega_m \equiv \dfrac{\rho_{m0}}{\rho_{cr}};
	\Omega_r \equiv \dfrac{\rho_{r}}{\rho_{cr}};
	\Omega_{\Lambda} \equiv \dfrac{\rho_{\Lambda}}{\rho_{cr}};
	\label{eqn:Omega}
\end{equation}

Using equation \ref{eqn:Omega} and \ref{eqn:rho_t}, we can rewrite equation \ref{eqn:fe1} 
as:

\begin{equation}
	E^2(a) \equiv \dfrac{H^2(a)}{H_0^2} = \left[\dfrac{\Omega_r}{a^4}
	 + \dfrac{\Omega_m}{a^3} +
	 \Omega_{\Lambda} - \dfrac{Kc^2}{a^2H_0^2}  \right]
\end{equation}

Now it is straightforward to calculate the age of the Universe at time $t$ from the big bang
when scale factor was $a(t)$,
\begin{equation}
	H(a) = \dfrac{\dot{a}}{a} = \dfrac{1}{a} \dfrac{da}{dt},
\end{equation}
\\
which gives,
\begin{equation}
	dt = \dfrac{da}{aH(a)},
\end{equation}
\\
and therefore,
\begin{equation}
	t(a) = \int_0^a \dfrac{da^{\prime}}{a^{\prime}H(a^{\prime})}.
\end{equation}
\\
This equation gives the age of the Universe when its size was
scaled to the scale factor $a$ with respect to the present. Hence, both $t$ and
$a$ defines the epoch in the cosmic history of the Universe.

%-----------------------------------------------
\subsection{Cosmological redshift}

Consider a source at time $t$ in the cosmic history emitting light reaching us today. 
Due to the expansion of the Universe, the wavelength of the light observed 
($\lambda_{\rm obs}$) will be different (stretched or redshifted) from the 
wavelength at which it was emitted ($\lambda_{\rm emit}$) by a factor by which
the Universe is expanded during the travel time of the photons, i.e.,
\begin{equation}
	(1+z) \equiv \dfrac{\lambda_{\rm obs}}{\lambda_{\rm emit}} = \dfrac{a_{\rm obs}}{a_{\rm emit}}
\end{equation}
\\
and as conventionally we define $a_{\rm obs} = a_{\rm today} = 1$, we have,
\begin{equation}
	(1+z) = \dfrac{1}{a(t)}
\end{equation}
\\
where, $z$ is known as the cosmological redshift of the source and also describe
the epoch in the cosmic history of the Universe. Therefore, the terms scale factor 
$a$ and redshift $z$ and cosmic time $t$ will be used interchangeably 
characterizing the epoch in the cosmic history of the Universe. 


%-----------------------------------------------
\subsection{Cosmological distances}
As the Universe is expanding with cosmic time, the notion of distance to a source
is not strict and characterizes conceptually. So if we go in the frame of reference
which is expanding with the Universe, or the so-called comoving coordinates, we can
define a comoving distance ($\chi$) as the distance to the radial coordinate of a source in
an expanding Universe,

\begin{equation}
	-\mathrm{d}\chi(a) = \dfrac{c\mathrm{d}t}{a} = 
	\dfrac{c\mathrm{d}a}{a \dot{a}} = \dfrac{c\mathrm{d}a}{a^2 H(a)}
\end{equation}
\\
or,
\begin{equation}
	\mathrm{d}\chi(z) = \dfrac{\mathrm{d}z}{H(z)} 
	\Rightarrow \chi(z) = \int_0^z \dfrac{\mathrm{d}z}{H(z)}
\end{equation}
\\
$\chi(z)$ is the radial comoving distance to redshift $z$. 

Suppose one knows the size ($d$) of a source at redshift $z$ and hence measure its
distance by measuring the angle subtending to the observer ($\theta$), this distance is referred
to as angular diameter distance and define as,
\begin{equation}
	D_{\rm ang}(z) = \dfrac{d}{\theta} = a(z)\chi = \dfrac{\chi}{1+z}
\end{equation}
\\
the multiplicative factor $a(z)$ is due to the fact that the Universe was smaller by 
a factor of $a$ when the light was emitted. 

Similarly, if one measure the flux ($S$) from a source and model its luminosity ($L$), it is
possible to infer the distance to the source referred as Luminosity distance and 
given by,
\begin{equation}
	D_{\rm lum}(z) = \sqrt{\dfrac{L}{4\pi S}} = \dfrac{\chi}{a(z)} = (1+z)\chi
\end{equation}
\\
the multiplicative factor of ($1/a(z)$) is due to the redshifted photons during the
expansion of the Universe.


%------------------------------------------------------------------------------
\clearpage
\section{Structure Formation in the Universe}

An absolutely homogeneous Universe would be very easy to write mathematics for, but 
would be boring. Thanks to the inhomogeneties in the Universe, we exist. The seeds
of those homogeneties were the tiny quantum fluctuation, which in our concordance 
model, is stretched up by the {\it inflation} and transform into small density
perturbations. As a result of gravitational instability, these small perturbations 
grow and form structures at various scales following baryonic physics. 
So, the history of the  structure formation in the Universe, as described in the 
concordance model can be studied in two parts: (i) {\it Linear theory}, when
the size of the perturbation were small and higher orders terms can be ignored and 
(ii) {\it Non-linear theory}, when the size of the perturbations grows significantly
large that higher order terms cannot be ignored.

Starting from CMB and Recombination era, talk about homogeneous Universe, matter-
dominated Universe, dark-ages etc.

Hierarchical structures, observational constraints, Re-ionization.

%----------------------------------------------
\subsection{Linear theory}


We start by defining the density contrast, the relative deviation of the density
from the mean background density of the Universe,

\begin{equation}
	\delta(r,t) = \dfrac{\rho(r,t) - \bar{\rho}(t)}{\bar{\rho}(t)},
\end{equation}
\\
where, $\bar{\rho}(t)$ is the mean matter background density 
of the Universe at that epoch. 
As suggested by the tiny fluctuations in the CMB temperature anisotropy, 
the perturbations in the density field at cosmological scales during 
the early times were small. Hence, the higher powers of $\delta$ terms
can be neglected and thus the predictions of linear perturbation theory
are valid at early times.

Assuming a matter dominated Universe, which is a good approximations at the
early times, with dark-matter as the dominant component being collisionless. 
So, we can approximate the matter as a pressureless fluid which is fairly 
valid at large scales. The fluid equations for vanishing pressure are: 
(i) Continuity equation, describing the conservation of matter; (ii) Euler
equation, describing the equation of motion for the fluid and finally (iii) 
Poisson equation describing the gravitational field. Solving these three 
equations in linear regime, Fourier space and comoving coordinates will give
the so-called {\it Growth equation}:

\begin{equation}
\centering
\dfrac{\partial^2\delta}{\partial t^2} + \dfrac{2\dot{a}}{a} 
		\dfrac{\partial\delta}{\partial t}
		- \dfrac{3H_0^2 \Omega_m}{2a^3} \delta = 0
\end{equation}
\\
This equation is fair approximation at large scales, where the perturbations
are small and completely describe the distribution and evolution of perturbations. 
The equation resembles a harmonic oscillator with damping
term govern by the Hubble parameter. The solution to this equation can be 
separated in spatial and temporal part, where the temporal part gives the 
growth of the structures, $D_+(t)$, and therefore,

\begin{equation}
	\dfrac{\partial^2 D_+(t)}{\partial t^2} + 2 H(t)
		\dfrac{\partial D_+(t)}{\partial t}
		- \dfrac{3H_0^2 \Omega_m}{2a^3} D_+(t) = 0
\end{equation}
\\
$D_+(t)$ is also known as the growth factor. It describes the evolution of the
density fluctuations with cosmic time. But to model the density fluctuation 
at any time, we need to describe the initial condition. It is not possible
to predict/describe the fluctuation at any given spatial coordinate in space,
however, it is more wise to study it statistically and describe the statistical
properties of the density field. 

The simplest of the statistics is the two-point function. Suppose there is a 
completely random distribution of galaxies in the Universe without any deterministic 
force. Now, if we ask what is the probability of finding a galaxy close to another
galaxy at a distance $r_1$ and at another distance $r_2$, it must be the same. 
However, due to Gravitational force, due to which galaxies attracts each other, 
there is this excess probability to find a galaxy closer to another galaxy than
further. So say, the probablity of finding another galaxy at $r_1$ is larger than
at $r_2$ if $r1<r_2$.

This excess probability can be modelled as the two-point correlation function $C(x,y)$ and define as, 
\begin{equation}
	\langle \delta(x) \delta^{\star}(y) \rangle = C(|x-y|)
\end{equation}
\\
The two-point correlation function (2PCF) is only the function of the distance
separation between two points in the space. It is more convenient to work in 
Fourier space and hence we define the Fourier transform of the 2PCF as the 
power spectrum ($P(k)$) of the density field,

\begin{equation}
		\langle \tilde{\delta}(k) \tilde{\delta}^{\star}(k^{\prime}) \rangle = 
				(2\pi)^3 \delta_D(k-k^{\prime}) P(k)
\end{equation}
\\
$k$ is the wavevector at the corressponding scale. So, once we define the initial
power spectrum, along with the growth factor $D_+(t)$, we have a complete description
of the density field. Assuming the initial density field as a realization of Gaussian
random field, the task is not that complicated. This assumption lead to a simplified
expression for the linear power spectrum,

\begin{equation}
	P_{\rm Lin}(k) = A k^{n_s} T^2(k)
\end{equation}
\\
where, $n_s$ is known as the spectral index of the density field and $A$ is the
normalisation factor. $T(k)$ is known as the transfer function and describes
the scale weighting when the perturbation at those scales enters the horizon
and becomes important. 

So, as we already know that the evolution of linear perturbation grows 
proportional to the growth factor, we can thus write,

\begin{equation}
	P_{\rm Lin}(k,z) = P_{\rm Lin}(k) D_+(z)
\end{equation}

The shape of the linear power spectrum is determined by the parameter $n_s$ and 
transfer function, however, there is a different form of normalisation in 
practice such that if one counts galaxies in sphere of radius 8 $Mpc/h$, then
the average relative error is close to unity. 

So, we define the variance of the smoothed density field as,

\begin{equation}
	\sigma^2(R) = \int \dfrac{d^3k}{(2\pi)^3} |\tilde{W}(k,R)|^2 P(k)
\end{equation}
\\
where, $\tilde{W}(k,R)$ is the Fourier transform of the tophat function smoothed 
at scale $R$ and given by,

\begin{equation}
	\tilde{W}(k,R) = 3\dfrac{\sin(kR) - kR \cos(kR)}{(kR)^3}
\end{equation}
\\
and therefore we have,

\begin{equation}
	\sigma^2(8 Mpc/h) \equiv \sigma_8^2 \approx 1
\end{equation}
\\
$\sigma_8$ is the parametrisation in practice to normalize the power
spectrum. 

\subsection{Zeldovich approximation}

In Lagrangian perturbation theory (LPT) \cite{} at large scales where matter is
well approximated as fluid, one can write the position each element of the fluid as
\begin{equation}
	x(q,t) = q + \Psi(q,t)
\end{equation}
\\
where, $q$ is the initial position of the element and $\Psi(q,t)$ is the Lagrangian
displacement field. $q$ and $\Psi(q,t)$ together completely describe the motion
of the cosmological fluid at any time. In LPT, one can try to find the solution 
perturbatively,

\begin{equation}
	\Psi(q,t) = \Psi^{(1)}(q,t)+\Psi^{(2)}(q,t)+\Psi^{(3)}(q,t)+\dots
\end{equation}

The first order solution ($\Psi^{(1)}(q,t)$) is the Zeldovich approximation (ZA). It
is an intuitive way to understand the filamentary structures in the cosmic web 
and understanding of non-linear structure formation. As only first order term
is considered in LPT, the approach is quasi-linear in nature.

There are particularly two advantages of using ZA instead of linear theory:

\begin{itemize}
	\item First, it is easy to include the redshift space distortions in this formalism,
	\item Second, Even though ZA is in a sense linear, it provides non-linear smearing
	to the BAO feature to the amount that matches the simulations very well. 
\end{itemize}

The Zeldovich power spectrum is given by (see e.g. \cite{})
\begin{align}
(2\pi)^3&\delta^D (k)+P(k)=\int d^3 q ~e^{-i\VEC{q}\cdot\VEC{k}}\nonumber\\
&\times\exp\left[-\frac{1}{2}k_ik_jA_{ij}(\VEC{q}))\right],
\label{eq:PSwithAW}
\end{align}
where 
\begin{equation}
A_{ij}(\VEC{q})=X(q)\delta^K_{ij}+Y(q)\hat{q}_i\hat{q}_j,
\end{equation}
and
\begin{align}
  X(q) =& \int_0^\infty \frac{dk}{2\pi^2} P_L(k)
  \left[\frac{2}{3} - 2 \frac{j_1(kq)}{kq}\right] , \\
  Y(q) =& \int_0^\infty \frac{dk}{2\pi^2} P_L(k)
  \left[-2 j_0(kq) + 6 \frac{j_1(kq)}{kq}\right] .
  \label{eq:XYex}
\end{align}
Here $P_L(k)$ is the linear power spectrum and $j_n$ is the spherical Bessel function of order $n$. 

%----------------------------------------------
\subsection{Non-Linear theory}

At late times or at small scales, the approximation $\delta << 1$ breaks. So, the analytical
description becomes difficult/impossible for the structure formation. The solution then would 
be to rely on simulations, higher order perturbation theory, physical approximations to the
distribution of the matter in the Universe or semi-analytic approaches. Analogous to these
approaches, there are various estimators for the power spectrum of the matter density field,
here we discuss two of them: N-body simulations (Numerical solution) and the halo model (
Analytic approximatoin).

\subsubsection{N-body simualtions}

With advancement in the compuatational power in the last decade, it is now possible to
simulate the Universe at cosmological volumes to high accuracy up to the scales where
only gravity is important in structure formation. These scales are highly non-linear and
very difficult to describe by higher order perturbation theories etc. An N-body simulation
characterizes to simulate a Universe consists of only dark-matter (and no baryons) that 
interacts only gravitationally. Such a simulation is useful in studying the
structure formation at scales large enough that baryonic processes are not 
important and small enough that the clustering processes are non-linear and difficult
to describe analytically. 

It is not possible (or at least very expensive) to simulate a volume equivalent to the
observable Universe, but only a part of it. Assuming that the Universe is homogeneous
at cosmological scales (or the scales of the largest structures BAO), the simulation
volume must at least include these structures. So a cube with side length nearly 200 Mpc/h
is needed to simulate a true representative of the Universe. Larger volumes are needed
for various other purposes, e.g., covariance matrix (to be discussed in section \ref{}).

One very important ingredient of a simulation is the initial conditions which are
set to very high redshift and let the particles evolve with gravity up to redshift zero. 
The best way to set the initial condition is to put all the particles in a uniform 3D grid
and then displace each particle with a displacement field to match the initial power 
spectrum (linear). 

Another important aspect of a simulation is its resolution. This depends on the number 
particles or the mass of each dark-matter particle (at least one of them is free parameter 
to fix at the beginning). Smaller is the mass of the individual dark-matter particle, 
larger will be the total number of particles in order to match the critical density of 
the Universe today and therefore, one can resolve smaller scale structures. However, 
there is always a trade-off between large volumes and higher resolutions. 

Some examples of top simulations here...Emulator, Skillman, Aurel's simulations etc. 



\subsubsection{The halo model}

The halo model is one of the more successful analytic and physical framework to describe
the clustering and growth of structures in the Universe. In this framework, all the matter
in the Universe is assumed to be in the form of spherical halos whose radius are defined by a 
density threshold, usually uses as the 200 times the mean matter density of the Universe 
(e.g., $R_{200}$). The  distribution of halos are assumed to follow a radial density 
profile, which depends on the mass of the halos, truncated at $R_{200}$ or at virial 
radius ($R_{\rm vir}$).

The assumptions of the halo model are invalid in details but are well approximated and 
hence the estimators for the matter power spectrum are well in agreement 
(nearly 20$\%$ at non-linear scales)
to the more accurate simulations. However, due to these invalid approximations, it is not
possible to achieve higher accuracy, like sub-percent, with this estimator. 

There are particularly four ingredients to model the power spectrum in halo model framework:

\begin{itemize}
	\item Linear power spectrum.

	\item Radial density profile of the halos. Usually for dark-matter only Universe, the so-called
			Navarro-Frenk-White (NFW) is used as a Universal profile completely characterized by the
			mass (or concentration) of the halo,
			\begin{equation}
				\rho(r) = \dfrac{\rho_s}{(r/r_s)(1+r/r_s)^2}
			\end{equation}
			\\
			where, $r_s$ and $\rho_s$ are the scale radius and characteristic density at scale radius
			respectively. Here we define the concentration parameter of the halo as,

	\item Mass function ($f(M)$): a functional form defining the number of halos as a function of halo
			mass. 

	\item Halo bias: As the halos are biased tracers of the matter, one important ingredient is 
			the description of the bias.  
\end{itemize}

Once we know these four ingredients, it is very straightforward to model the power spectrum. For 
more details see chapter \ref{}

\subsection{Baryonic contributions}
It is relatively easy to model and describe the clustering of matter that interacts with only 
one force (gravity) aka dark-matter. But we exist and not made up of dark-matter, instead by the
ordinary matter we know as baryons which interacts other than gravity too and exhibit pressure. 
Even the Universe with baryons and dark-matter would behave very similar at large scales,
but at small scales where the baryonic processes are important, dark-matter only Universe is
not a good description. 

To model the structure formation with baryons, is rather a messy task. Analytic calculations
can be very complicated and not-reliable. However, simulations are better ways. One can always
put baryonic matter along with dark matter in the simulations and evolve governing gravity and
baryonic processes which are known to be important. Some of the processes are described below,

\begin{itemize}
	\item Star formation
	\item Feedback processes
	\item Radiative transfer
	\item Adiabatic contraction/expansion
\end{itemize}

When there is very little understanding of start formation processes, other processes can be 
modelled with some approximations. But carrying out these calculations in big cosmological 
volumes are very expensive and may still be out of the reach to the current computational 
abilities. 

The other way is to include the baryonic processes is to perturb the density profile 
of the halos with baryonic processes and model with the halo model. This approach can 
be calibrated with simulations or observations but still remain analytical with some 
free parameters to be measured by data directly. For a similar approach see chapter \ref{}



\subsection{Covariance matrix of matter power spectrum}
As the 2PCF and power spectrum are statistical quantities, there is a statistical limit
to which it can be measured in cosmological surveys or simulations. This limit is expressed
as the covariance matrix. 

The full covariance matrix of the matter power spectrum consist of three broad parts:

\begin{equation}
	C(k,k^{\prime}) = C^{G}(k,k^{\prime}) + C^{NG}(k,k^{\prime}) + C^{SSC}(k,k^{\prime})
\end{equation}
\\
where, the three terms on the right are the Gaussian, Non-Gaussian and super-sample covariance contributions. 

\begin{itemize}
	\item Gaussian part $C^{G}(k,k^{\prime})$: The matter power spectrum is computed
			by averaging over the modes in spherical shells in the Fourier space. It 
			is understood that if one average over more number of modes, the underlying
			error on the average will be smaller and vice-versa. So, the Gaussian part
			of the covariance matrix gives this contribution and hence have Poissonian
			structure. 
			\begin{equation}
				C^{G}(k,k^{\prime}) = \dfrac{2}{N(k)} \delta_{k,k^{\prime}} P^2(k)
			\end{equation}
			\\
			where, $N(k)$ is the number of modes corressponding the wavevector $k$. One 
			can either count the number of modes while averaging over the power spectrum
			or can compute as,
			\begin{equation}
				N(k) = 4 \pi k^2 dk \left( \dfrac{L}{2\pi} \right)^3
			\end{equation}
			\\
			where, $L$ is the box size and $dk$ is the bin width. The Gaussian part 
			contributes only to the Gaussian part of the covariance and gives no 
			information about the correlated errors between different modes. 

	\item Non-Gaussian part, $C^{NG}(k,k^{\prime})$: The non-Gaussian covariance term is 
			is induced by the non-linearity and arising from the connected 4-point function or the trispectrum. This term gives the correlated errors between
			different wavevectors and also contributes to the off-diagonal terms
			of the covariance matrix. 

	\item Super-sample covariance (SSC), $C^{SSC}(k,k^{\prime})$: The SSC is the sampling
			error due to the coupling to the modes larger than the size of the survey or
			simulation. Because this contribution appear constant as the survey scale, it 
			can be viewed as a curvature term to the survey which can further be mimic
			by the change in background density. Therefore this term can be modelled
			completely by the response of the change in background density to the
			matter power spectrum,
			\begin{equation}
				C^{SSC}(k,k^{\prime}) = \sigma_b^2 \dfrac{\partial P(k)}{\partial \delta_b}
										\dfrac{\partial P(k^{\prime})}{\partial \delta_b}
			\end{equation}
			\\
			where, $\sigma_b$ is the linear variance. 
\end{itemize}

The accurate quantification of all these terms is crucial in order to perform any
likelihood analysis based on observables modelled on matter power spectrum. As we
advance in the quality of data, the smaller scales gives more constraining power 
over cosmological parameter space, the contributions from non-Gaussian part and SSC
becomes very important and ignoring them may overestimates the constraints on the 
cosmological parameters or bias and mislead the interpretation. 





%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
\clearpage
\section{Probes of Cosmology}

Starting general ideas and all possible probes of cosmology with preferences.
\begin{itemize}
	\item CMB
	\item BAO
	\item Supernovae-Ia
	\item Galaxy cluster
	\item Redshift space distortions
	\item Gravitational lensing
\end{itemize}


%------------------------------------------------------------------------------

\subsection{Gravitational Lensing}
The bending of light from a distant source due to the gravitational field of the
intervening matter is known as
the gravitational lensing (or lensing henceforth). It is a powerful technique to 
obtain information about the intervening matter (or the lens from now). There are
various observables due to this phenomenon like multiple-images of the source, time 
delays amongst the multiple-images, shape distortion of the source, change in brightness
of the source etc. Each of these observables can be utilised to infer or model various
aspects: from the mass distribution of the lens to cosmology. As lensing phenomenon depends 
on the total mass of the lens and not on its nature (dark-matter or baryonic), it is
so far the most unbiased technique to probe the distribution of matter in the Universe
and to do cosmolgoy.

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{figures/lens_geometry.png}
	\caption{}
	\label{fig:lens}
\end{figure}

Consider figure \ref{fig:lens}: a ray of light from a source, at a distance $D_s$ from
the observer, deflected due to the lens, at a distance $D_d$ from the observer,  by an 
angle $\hat{\alpha}$ and seen
by the observer at an angle $\theta$ from the lens. If there were no lens, it would have
been observed at $\beta$. Employing small angles approximations, 

\begin{equation}
	\beta  = \theta  - \dfrac{D_{ds}}{D_s} \hat{\alpha}(\xi) \equiv \theta - \alpha(\xi)
	\label{eqn:lens}
\end{equation}
\\
where, $\alpha$ is the scaled deflection angle with respect to the center of the 
lens to the observed position of the source image and $\xi$ is the impact parameter
of the image on the lensplane. The equation \ref{eqn:lens} is known
as the lens equation. The equation describes the true position of the source at $\beta$ to
be seen at $\theta$ due to the lens which is deflecting the light rays by an angle $\alpha$.
If this equation has more than one solution, there will be multiple images of the source. 

The deflection angle $\hat{\alpha}$ depends on the impact parameter as well as the 
mass/density distribution of the lens. For a spherically symmetric lens, general relativity
predicts,
\begin{equation}
	\hat{\alpha} = \dfrac{4GM}{c^2 \xi}
\end{equation}
\\
which is twice the value predicted by Newtonian gravity. For more complex lenses, the 
relation between the deflection angle, impact parameter (or $\theta$) and the mass
distribution of the lens can be obtained under the so-called geometrically thin lens
approximation as,

\begin{equation}
	\alpha(\theta) = \dfrac{1}{\pi} \int d^2\theta^{\prime}
						\kappa(\theta^{\prime})
						\dfrac{\theta-\theta^{\prime}}{|\theta-\theta^{\prime}|^2}
	\label{eqn:alpha}
\end{equation}
\\
where, $\kappa(\theta)$ (or the convergence) is the surface mass density in the 
units of a critical density $\Sigma_{\rm cr}$ defined as,

\begin{equation}
	\kappa(\theta) \equiv \dfrac{\Sigma(\theta)}{\Sigma_{\rm cr}}; 
	\Sigma_{\rm cr} = \dfrac{c^2}{4\pi G} \dfrac{D_s}{D_d D_{ds}}.
\end{equation}

The critical density completely depends on the geometry of the system. For generating
multiple images, $\kappa>1$ and hence the critical density is a characteristic 
surface mass density in order to produce extra images given a source in the background.
Therefore, critical density is also differentiate between region
of, so-called, strong and weak lensing, which are described in more details below.

%----------------------------------------------
\subsubsection{Strong Gravitational Lensing}

Strong gravitational lensing (or SL henceforth), referred to the lensing phenomenon
when the strength of the gravitational field of the lens is sufficiently strong and 
also the alignment between the source, the lens and the observer is optimal enough to
produce multiple images of the source. From equation \ref{eqn:alpha}, SL regime marked
by $\kappa >1$ and relatively small values of $\theta$. So, the multiple images of a
background sources contains information about the projected mass distribution of the lens. 

Given the mass distribution of a lens ($\kappa(\theta)$) and position of the source,
it is straightforward to find out the position of the multiple images, this process
is called forward modelling. But given a set of multiple images, which are the main 
observables in SL, it is a highly degenerate process to reconstruct the mass
distribution of the lens. The procedure is referred as lens inversion. There are 
several methods for lens inversion. Many of those procedures put strong constraints
on the mass distribution of the lens and assume a functional form and hence are called
as parametric methods. These methods mainly involve to find the optimal values 
of the free parameters of the functional form of the mass distribution of the lens. 
For example \cite{}, \cite{},\cite{}.

There are some methods which invert the lens with minimal assumptions and without
assuming any prior form of the mass distribution of the lens, and are referred as
non-parametric methods. In these methods, the number of free parameters are often
too large, usually some building block of the total mass maps, and hence larger
statistical consideration is needed. For example, \cite{}, \cite{}.

Where parametric models are more efficient, non-parametric models are more
accurate and unbiased. In this work we used a non-parametric lens inversion
library called as GRALE \cite{} to trace the mass distribution of some very massive
galaxy clusters. The building blocks of the mass map are the Plummer spheres, where
other choices such as squares and Gaussian spheres are also available in the library, 
and the total mass map is the super position of these building blocks. GRALE uses
a genetic algorithm to find optimal solution to the weights of the building 
blocks and the resolution of the mass maps is adaptively increased (or decreased)
For more detailed documentation see \cite{}.

Another observable in SL is the time delays between multiple images
of the source. The light rays from the same source travel different directions
and due to the curvature of space time, depending upon the mass distribution 
of the lens, they travel different distances before they reach the same 
observer. And because the distances in cosmology depends on the Hubble constant
or the expansion rate of the Universe, one can calculate the value of the
Hubble constant given measured time delays and mass distribution of the source. 
However, it is also possible to have the Hubble constant given and put
additional constraints on the mass distribution of the lens using measured
time delays. 


%----------------------------------------------
\subsubsection{Weak Gravitational Lensing}

Weak gravitational lensing (or WL henceforth) referred to the phenomenon when 
the lensing is not strong enough to produce multiple images but strong enough
to distort the shape of the source. The deformation of the shape of the observed galaxies
due to the intervening matter is referred to as cosmic shear. This
signal is very small, nearly 1-2$\%$ of the intrinsic ellipticity of the source 
and can only be measured statistically under the assumption that the intrinsic 
ellipticity of the background galaxies do not have a preferred direction. If one 
measures the cosmic shear of all background sources behind a lens or mass concentration,
it tends to align towards the centre of the mass concentration. Hence, the shear
two point correlation function can be directly related to that of the convergence
or the projected density,

\begin{equation}
	P_{\kappa}(\ell) = \int_0^{\chi_H} \dfrac{g(\chi)^2}{\chi^2} P(\dfrac{\ell}{\chi},\chi) d\chi
\end{equation}
\\
where,
$\chi$ is the comoving distance to the source and $\chi_{H}$ is the comoving distance to the
horizon. $\ell$, defined as $\ell = 180/\theta$, is the multipole and give the angle in the sky. 
$\g(\chi)$ is the lensing weight given by,

\begin{equation}
	g(\chi) = \dfrac{3\Omega_m}{2H_0^2} \dfrac{\chi}{a(\chi)\bar{n}} \int_{\chi}^{\chi_(H)}
					n(\chi^{\prime}) \dfrac{\chi(\chi^{\prime}-\chi)}{\chi^{\prime}}d\chi^{\prime}
\end{equation}
\\
where, $n(\chi)$ gives the distribution of sources as a function of comoving distance or redshift.
This can be fixed for a given survey and expected lensing weights can be computed. Which then
be convolved the the matter power spectrum and projected along the line of sight in order
to model the power spectrum of the projected mass density or the convergence. 

%----------------------------------------------
\subsubsection{Microlensing}



%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
\clearpage
\section{Essential statistics for cosmology}
There are two broad schools in statistics Frequentist and Bayesian. Frequentists believe
that parameters are all fixed and unknown constants in the model whereas Bayesians
believe that parameters are random variables that has a given distribution, and other probability
statements can be made about them. According to Frequentists, there are TRUE
population parameters that are unknown and can only be estimated by the data whereas,
Bayesians believe that only data are real; the population parameters are an abstraction, and
as such some values are more believable than others based on the data and on prior beliefs.
In other words, Bayesian defines the probability as the degree of belief where Frequentists
termed it as frequency of occurrence in a set of trials. 

\subsection{Statistical Inference}



\subsection{MCMC and Fisher}
\subsection{Genetic algorithm}

%------------------------------------------------------------------------------
%------------------------------------------------------------------------------
\clearpage
\section{Motivation}

The theory of structure formation is partially understood --  at large scales 
or at early times when the perturbations were tiny or only linear order is important. 
However, for small scale clustering processes, when the higher order in perturbations 
become important the analytic solutions are not possible, we rely on simulations and 
approximations. 

The main motivation of this work is to model the distribution of matter in high mass
haloes like lensing clusters and in the Universe in general. 

This information is useful in two ways: 

First, the accurate modelling of the matter distribution in individual clusters
give information about the properties of dark-matter. On large scales, dark-matter
is known to be collisionless and non-interacting except for its gravitational effects,
but it is important to quantify and test the hypothesis in high dense regions, like
at the center of clusters. A small but finite cross section of dark-matter particles
can be well tested in high dense regions, may have stronger implications in our 
understanding of the properties of dark-matter and the Universe in general.  
Also, as the baryonic processes and theory of galaxy
formation is poorly understood, the central regions of the galaxy clusters can be 
used as laboratories to study these processes. It can be done only if the matter 
distribution in the individual systems is well constrained. 
Further, as in the hierarchical structure formation, many small haloes interacts and
merge to form large collapsed virialised haloes, the study of the distribution of 
matter in lensing clusters at high redshift give information about the merging
history/stage of the cluster. 

The modelling of the distribution of matter in the Universe is very important
in order to model the cosmological observables. Statistically, the modelling of the
matter power spectrum or 2PCF is important if one wants to do cosmology because 
the matter power spectrum underlies many
cosmological observables like BAO, weak lensing, galaxy clustering, redshift
space distortions etc. Secondly, the modelling of the 
covariance matrix of the matter power spectrum is vital in order to do correct
likelihood analysis and testing cosmological models. Finally baryonic physics 
also changes the power spectrum at small scales, if neglected, it will add biases
in the cosmological parameters and mislead the interpretations. 

\subsection{Challenges}

There are certain challenges in order to accurately model the mass distribution
in clusters and in the Universe. 

\begin{itemize}
	\item Distribution of matter in clusters: Mass is not the observable, what we 
			observe is the light in different frequency bands. We can derive 
			redshifts, velocity dispersions etc from this. To relate these observables
			to mass distributions, we have approximations and various models which
			are often rich in systematics, i.e., the incomplete understanding, and 
			biases. Gravitational lensing is far the most unbiased technique in order
			to trace matter in lenses. However, even in lensing the inversion of the lens,
			is a highly degenerate process. Often and very much in 
			practice, people assume that mass follows light, which is a good approximation
			but may not be true everywhere. So, this is essential that the 
			mass reconstruction techniques are independent of such assumptions in order
			to learn about intrinsic properties of the dark-matter and baryonic
			process in the high dense regions of the lensing clusters.

	\item Modelling matter power spectrum and its covariance matrix: In order to model
			the matter power spectrum at non-linear scales, simulations are far the 
			best solution as all other analytic approaches are difficult. But this is limited
			by the volume and resolution of the simulation. Also a good simulation
			can be very expensive computationally. So, it is possible to simulate a big
			volume at very good resolution for a cosmological realization, but in order
			to carry out a likelihood analysis on some cosmological data, simulations are
			very expensive and impossible to do. So, we rely on semi-analytic models, 
			which can model the matter power sepctrum with some function of cosmology
			and are more accurate than the perturbation theories etc at non-linear 
			scales. Similarly, in order to get good covariance matrix of the matter power
			spectrum from simulation, we need to simulate a volume of the order 1000 Gpc/h
			cube, which again is very expensive specially when small scale modelling 
			is necessary. 

	\item Modelling baryonic physics in two-point functions: Finally baryonic effects are 
			important at small scales statistically too. If these effects are not present
			in the model of the power spectrum, it will bias the whole exercise and
			the recovered cosmology might be very precise but not accurate. 
\end{itemize}

So, the main goal of this work is to target these challenges. We perform and completed
a number of projects in order to achieve these goals. 


\subsection{Projects in this work}

In this PhD dissertation, I (along with other collaborators in different projects) tried
to answer few of these question and build a better understanding of distribution of 
matter in the Universe and its clustering properties. 

The very first project is a mass modelling problem in strong lensing cluster. Here we
used a publicly available code GRALE, modified it for an optimum solution and resolution,
and put tight constraints on the mass distribution of few lensing clusters. By studying
the mass and light distribution of these clusters, we imply that it is possible 
that the dark-matter has a finite self-interaction cross section which is large and
important in central parts of clusters. 

In a similar project, we also show that time delay information is very useful in 
order to put additional constraints on the central region of the clusters which can't 
be resolved directly, specially when the steepness degeneracy is broken by the 
presence of background sources at different redshifts.

We also presented a (semi) analytic model for the matter power spectrum, which is
computationally inexpensive and compute the power spectrum to a percent level 
accuracy up to k $\sim$ 1 h/Mpc. The motivation of this model is the halo model
and we also derived simple form of the covariance matrix. We proposed a way to 
marginalise over baryonic effects. 

In a similar project to the previous one, we used the halo model in order to directly
model the effects of baryons on the matter power spectrum and to the extention on the
weak lensing shear-shear power spectrum. The effects are small at comparatively large
scales but as the next generation surveys are expected to measure these quantities 
to very small scales, the baryonic effects are important to take into account. If not, 
it will add biases to the cosmological parameters up to 10 sigma. 

\subsection{A unified picture}

If we try to draw a bigger picture from all the projects above, it states that
baryonic physics is very important to model in order to understand the clustering processes,
galaxy formation as well as to do cosmology with future generation surveys. 

Gravitational lensing is one ideal tool to do so. Strong lensing
is very useful in studying the individual systems. 
Weak lensing shear measurements
on larger area in the sky is the most promising tool to do cosmology under
control systematics. One of the biggest source of systematics is again the 
baryonic physics at small scales and there are various ways to handle them. But
they can't be ignored. Precision cosmology and detailed information about the 
individual systems are needed in order to gain full understanding of the 
galaxy formation processes and evolution of the Universe. 






%------------------------------------------------------------------------------

\clearpage
\cite{2014MNRAS.440.2290M}

\cite{2014MNRAS.439.2651M}

\cite{2014A&A...567A..65B}

\cite{2014MNRAS.445.3382M}

\cite{2014arXiv1410.6826M}

\cite{2015PASJ...67...21M}

\cite{2015arXiv150403388M}





